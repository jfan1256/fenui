{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b3e372-90c5-421f-bdc4-7e0cad8aa74b",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80cf481a-7603-4022-9fac-b1298e65ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests \n",
    "import tiktoken\n",
    "import ray\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from math import ceil\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "from utils.system import *\n",
    "from class_data.data import Data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c2f14-7dd5-4e85-b3e5-f49c5dcde724",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ce752-ef28-4fa6-99c5-d4e08a2c985f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collect = []\n",
    "for i in range(1, 6):\n",
    "    collect.append(pd.read_parquet(get_format_data() / 'art' / f'wsj_art_{i}.parquet.brotli'))\n",
    "wsj_multiple = pd.concat(collect, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2567d9-9a23-46b2-99c3-a560b22da3e6",
   "metadata": {},
   "source": [
    "### Parallelized: Get number of tokens (per article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52d40c-4445-4438-b828-ac40f7baca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def get_token_count(article_text, encoding_param):\n",
    "    encoding = tiktoken.get_encoding(encoding_param)\n",
    "    token_count = len(encoding.encode(article_text))\n",
    "    return token_count\n",
    "\n",
    "def process_tokens_in_batches(df, column_name, encoding_param, batch_size):\n",
    "    num_batches = np.ceil(len(df) / batch_size)\n",
    "    all_token_counts = []\n",
    "    print(f\"Number of batches: {int(num_batches)}\")\n",
    "\n",
    "    for i in range(int(num_batches)):\n",
    "        print(f\"Processing batch: {i + 1}/{int(num_batches)}\")\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batch = df[column_name][start_index:end_index]\n",
    "\n",
    "        # Start asynchronous tasks for the batch\n",
    "        futures = [get_token_count.remote(text, encoding_param) for text in batch]\n",
    "        token_counts = ray.get(futures)\n",
    "        all_token_counts.extend(token_counts)\n",
    "\n",
    "    # Assign the token counts back to the DataFrame\n",
    "    df['n_tokens'] = all_token_counts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7939396-bc21-4cf6-ba14-2c71fbb9c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_encoding = \"cl100k_base\" \n",
    "max_tokens = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961127bd-50dc-4ac8-9f09-fff4b63c60b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "\n",
    "# Process articles in batches\n",
    "ray.init(num_cpus=16, ignore_reinit_error=True)\n",
    "start_time = time.time()\n",
    "wsj_multiple = process_tokens_in_batches(wsj_multiple, 'body_txt', embedding_encoding, batch_size)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total time to get all tokens: {round(elapsed_time)} seconds\")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2ee27-74fa-4d94-9dd8-094a4fcdb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "print(f\"Length before: {len(wsj_multiple)}\")\n",
    "wsj_multiple = wsj_multiple[wsj_multiple.n_tokens <= max_tokens]\n",
    "print(f\"Length after: {len(wsj_multiple)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1f3b0-5c7c-4a24-93f2-d5e110dc269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Data\n",
    "chunks = np.array_split(wsj_multiple, 8)\n",
    "for i, df in enumerate(chunks, 1):\n",
    "    print(i)\n",
    "    df.to_parquet(get_format_data() / 'token' / f'wsj_tokens_{i}.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4170c5d-0d02-408d-87b3-fc10660f9083",
   "metadata": {},
   "source": [
    "### Parallelized: Get embeddings (per array of article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d7181d8-a85c-4f6a-b798-394ded86acbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load articles\n",
    "wsj_multiple = Data(folder_path=get_format_data() / 'token', file_pattern='wsj_tokens_*').concat_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d3d9ba2-cae2-47b5-b7d0-9bc17941d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_multiple_token = wsj_multiple.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a70fbfe6-d992-42fa-a262-ee9e0a941b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def get_embedding_article(articles, model):\n",
    "    embeddings = model.encode(articles)\n",
    "    return embeddings\n",
    "\n",
    "def process_articles_in_batches(df, column_name, model, batch_size, article_size, delay_per_batch):    \n",
    "    num_batches = np.ceil(len(df) / batch_size)\n",
    "    times = []\n",
    "    \n",
    "    for i in range(int(num_batches)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if the batch has already been processed\n",
    "        save_path = get_format_data() / 'mxbai' / f'wsj_emb_mxbaiembedlargev1_{i}.parquet.brotli'\n",
    "        if save_path.exists():\n",
    "            print(f\"Skipping batch {i + 1}/{int(num_batches)} (already processed)\")\n",
    "            continue\n",
    "\n",
    "        # Get batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = min(start_index + batch_size, len(df))\n",
    "        batch_texts = df[column_name][start_index:end_index]\n",
    "        \n",
    "        # Group texts into sub-batches of size article_size\n",
    "        sub_batches = [batch_texts[j:j+article_size].tolist() for j in range(0, len(batch_texts), article_size)]\n",
    "        \n",
    "        # Start asynchronous tasks for each sub-batch\n",
    "        futures = [get_embedding_article.remote(sub_batch, model) for sub_batch in sub_batches]\n",
    "        embeddings_lists = ray.get(futures)\n",
    "        \n",
    "        # Convert embeddings to the desired format (list of embeddings)\n",
    "        embeddings_formatted = [embedding.tolist() for sublist in embeddings_lists for embedding in sublist]\n",
    "\n",
    "        # Save Batch\n",
    "        print(f\"Saving progress to {save_path}\")\n",
    "        all_indices = df.index[start_index:end_index].tolist()\n",
    "        # Create a DataFrame with a single column for embeddings\n",
    "        embeddings_df = pd.DataFrame({'embedding': embeddings_formatted}, index=all_indices)\n",
    "        embeddings_df.to_parquet(save_path, compression='brotli')\n",
    "        print(\"Progress saved\")\n",
    "        \n",
    "        # Time taken for the batch\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        times.append(batch_time)\n",
    "        \n",
    "        print(f\"Batch {i + 1}/{int(num_batches)} processed in {batch_time:.2f} seconds\")\n",
    "        \n",
    "        # Calculate and print estimated time to finish\n",
    "        avg_time_per_batch = np.mean(times)\n",
    "        batches_left = int(num_batches) - (i + 1)\n",
    "        estimated_time_left = avg_time_per_batch * batches_left\n",
    "        hours, rem = divmod(estimated_time_left, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(f\"Estimated time to finish: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "        \n",
    "        if delay_per_batch > 0:\n",
    "            time.sleep(delay_per_batch)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2689d29c-f265-441b-913d-a07196020cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 22:45:03,712\tINFO worker.py:1507 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 1/166216 (already processed)\n",
      "Skipping batch 2/166216 (already processed)\n",
      "Saving progress to C:\\Jonathan\\QuantResearch\\AlgoTradingModels\\fenui\\data\\format\\mxbai\\wsj_emb_mxbaiembedlargev1_2.parquet.brotli\n",
      "Progress saved\n",
      "Batch 3/166216 processed in 27.63 seconds\n",
      "Estimated time to finish: 1275h 48m 52.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "batch_size = 5\n",
    "delay_per_batch = 0\n",
    "article_size = 10\n",
    "\n",
    "# Process articles in batches\n",
    "ray.init(num_cpus=16, ignore_reinit_error=True)\n",
    "\n",
    "start_time = time.time()\n",
    "process_articles_in_batches(wsj_multiple_token, 'body_txt', model, batch_size, article_size, delay_per_batch)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total time to get all embeddings: {round(elapsed_time)} seconds\")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39f7e99b-52b9-49c9-ba3b-c4de5adda082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 244.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load articles\n",
    "test = Data(folder_path=get_format_data() / 'mxbai', file_pattern='wsj_emb_mxbaiembedlargev1_*').concat_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrativezoo",
   "language": "python",
   "name": "narrativezoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
